2017-07-27
------------------------------------------------------------------------
Implemented the following functions:

initialize_fft_phi  (fft_mkl.f90)
fft_phi             (fft_mkl.f90)
finalize_fft_phi    (fft_mkl.f90)

spat_to_SH_parallel (shtns.f90)

spat_to_SH_parallel in shtns.f90, calls the spat_to_SH, then calls the 
spat_to_SH_ml in a loop and compare the results. At this moment this is 
merely for debugging, but later I'll clean it up.

fft_phi performs n_theta_max times a 1D DFT on an (n_phi_max,n_theta_max)
input, with a (m_max, n_theta_max) output.

The spat_to_SH_parallel is particularly verbose at the moment (output
file is now 500MB more or less!!) but as mentioned, this is just for 
debugging. In draco:

srun -n 8 --label ${binary} n_procs_r=4 n_procs_theta=2 ${namelist_file}) > output.txt
cat ./output.txt | grep "^0:" | head -n 1000


2017-07-21
------------------------------------------------------------------------
Currently compiling SHtns with:
./configure --enable-mkl --enable-magic-layout --disable-openmp --prefix=$HOME/local/
make
make install

Use nm ~/local/lib/libshtns.a to see all the comprised functions.

Currently compiling magIC with:
cd build
rm -Rf ./*
cmake .. -DUSE_SHTNS=yes -DUSE_OMP=no
make VERBOSE=1

Test magIC with
cd ./


2017-07-20
------------------------------------------------------------------------
Functions from SHtns relevant for the parallelization (according to 
Thomas):

⚪ void spat_to_SH_ml (shtns_cfg, int im, cplx *Vr, cplx *Ql, int ltr)
⚪ void SH_to_spat_ml (shtns_cfg, int im, cplx *Ql, cplx *Vr, int ltr)
⚪ void spat_to_SHsphtor_ml (shtns_cfg, int im, cplx *Vt, cplx *Vp, cplx *Sl, cplx *Tl, int ltr)
⚪ void SHsphtor_to_spat_ml (shtns_cfg, int im, cplx *Sl, cplx *Tl, cplx *Vt, cplx *Vp, int ltr)
⚪ void SHsph_to_spat_ml (shtns_cfg, int im, cplx *Sl, cplx *Vt, cplx *Vp, int ltr)
⚪ void SHtor_to_spat_ml (shtns_cfg, int im, cplx *Tl, cplx *Vt, cplx *Vp, int ltr)
⚪ void spat_to_SHqst_ml (shtns_cfg, int im, cplx *Vr, cplx *Vt, cplx *Vp, cplx *Ql, cplx *Sl, cplx *Tl, int ltr)
⚪ void SHqst_to_spat_ml (shtns_cfg, int im, cplx *Ql, cplx *Sl, cplx *Tl, cplx *Vr, cplx *Vt, cplx *Vp, int ltr)

Currently called functions in magIC:
⚪ void spat_to_SH (shtns_cfg shtns, double *Vr, cplx *Qlm)
⚪ void SH_to_spat (shtns_cfg shtns, cplx *Qlm, double *Vr)
⚪ void spat_to_SHsphtor (shtns_cfg, double *Vt, double *Vp, cplx *Slm, cplx *Tlm)
⚪ void SHsphtor_to_spat (shtns_cfg, cplx *Slm, cplx *Tlm, double *Vt, double *Vp)
⚪ void SHsph_to_spat (shtns_cfg, cplx *Slm, double *Vt, double *Vp)
⚪ void SHtor_to_spat (shtns_cfg, cplx *Tlm, double *Vt, double *Vp)
⚪ void spat_to_SHqst (shtns_cfg, double *Vr, double *Vt, double *Vp, cplx *Qlm, cplx *Slm, cplx *Tlm)
⚪ void SHqst_to_spat (shtns_cfg, cplx *Qlm, cplx *Slm, cplx *Tlm, double *Vr, double *Vt, double *Vp)

Function call is practically the same, except that second argument is 
the m index (im), and the last argument is the ltr (which is lmax, 
obtained from shtns_calc_nlm()). Also, the V, Q, S and T are all COMPLEX for the 
_ml functions, because no FFT is performed for them.

Relevant dimensions from magIC:
⚫ lmP_max=lm_max + n_m_max                   @truncation:95
⚫ nrp = n_phi_max  , used for real fields    @truncation:102 (for SHTNS only)
⚫ ncp = n_phi_max/2, used for complex fields @truncation:106 (for SHTNS only) 
⚫ nfs = n_theta_max                          @blocking:269   (for SHTNS only)

Relevant fields from magIC:
⚫ general_arrays_t%*  REAL  nrp×nfs     allocated@get_nl:79~93
⚫ nonlinear_lm_t%*    CMPLX lmP_max     allocated@get_td:67~80

Initialization of shtns:
Config_0:
⚫ call shtns_set_size(l_max  , m_max/minc, minc, SHT_ORTHONORMAL + SHT_NO_CS_PHASE)
Config_1:
⚫ call shtns_set_size(l_max+1, m_max/minc, minc, SHT_ORTHONORMAL + SHT_NO_CS_PHASE)

Notation Conversion:
✘ shtns's lmax => magIC's l_max (+1 for Config_1)
✘ shtns's mmax => magIC's m_max/minc
✘ shtns's mres => magIC's minc

2017-07-18
------------------------------------------------------------------------
Total change of plans after last conversation with Thomas.

The parallel implementation is to be taken from the _shtns version
of the rIterTheta. SHtns library already supports the transforms
to a specific m and a specific l range, meaning that it should be
trivial to simply call those routines.

So now rIterThetaParallel is a copy of rIterThetaBlocking_shtns
merged with rIteration (to get rid of the transform_to_grid_space
defined in the later module).

2017-06-21
------------------------------------------------------------------------
Module legendre_grid_to_spec will be dropped in favour of 
legendre_parallel for MPI version. I'll copy the functions as they are
needed and adapt them accordingly.

legendre_spec_to_grid module is overcomplicated because of the blocking
nThetaStart < nTheta < nThetaStart-1+sizeThetaB (implemented to favour 
cache-hit). 

Dunno if I should keep it or throw it all away. Cache-Hit will be 
secondary if it incurs in extra allreduces, and if I understood 
correctly, this is basically a whole bunch of inner-products (which 
require an allreduce in parallel cases).

Maybe for the MPI sake it would be better to group the leg_helper
vectors into a matrix and assign them pointers e.g. 
leg_helper%matrix(:,sR) instead of leg_helper%sR.

This would turn lots of things into a simple matrix-vector product,
and therefore linearly scalable. This is all speculation, maybe it will
be better to keep current structure.



2017-06-02
------------------------------------------------------------------------
The callstack for the do_iteration, starting from rIterThetaParallel is:

⚪ do_iteration_ThetaParallel (rIterThetaParallel:450)
⚪    loop                (rIterThetaParallel:557)
⚪    this%transform_to_grid_space (rIterThetaParallel:154)
⚪       legTFG           (legendre_spec_to_grid:27)
⚪          leg_helper_t%__ATTRIBUTES__    
⚪       loop             (legendre_spec_to_grid:181)
⚪       legTFGnomag      (legendre_spec_to_grid:733)
⚪          leg_helper_t%__ATTRIBUTES__    
⚪       fft_thetab       (fft:156)
⚪         fftJW          (fft:169)
⚪           fft99aJW     (fft:383)
⚪           wpass2JW     (fft:483)
⚪           wpass3JW     (fft:518)
⚪           wpass4JW     (fft:621)
⚪           wpass5JW     (fft:729)
⚪           fft99bJW     (fft:426)
⚪       v_rigid_boundary (nonlinear_bcs:195)
⚪    this%gsa%get_nl                      ?
⚪    this%transform_to_lm_space           ?
⚪    get_br_v_bcs                         ?
⚪    get_lorentz_torque                   ?
⚪    courant                              ?
⚪    graphOut_mpi                         ?
⚪    probe_out                            ?
⚪    get_helicity                         ?
⚪    get_visc_heat                        ?
⚪    get_nlBLayers                        ?
⚪    get_fluxes                           ?
⚪    get_perpPar                          ?
⚪    store_movie_frame                    ?
⚪    get_dtBLM                            ?
⚪    getTOnext                            ?
⚪    getTO                                ?
⚪    this%nl_lm%get_td                    ?
⚪    getTOfinish                          ?
⚪    get_dH_dtBLM                         ?



2017-05-31
------------------------------------------------------------------------
 ✔ Adding rThetaParallel module; at the moment, it merely merges
   rThetaBlocking with rThetaBlocking_seq. I will slowly adapt it from
   now on
 ✔ I'm validating the modifications with the following command
      ./magic_wizard.py --link --use-mpi --mpicmd=srun --extracmd="n_procs_r=4 n_procs_theta=2 n_procs_m=2" --nranks=16 --name=couetteAxi
   called from samples/ directory. The modules/environment variables are:
      export FC=mpiifort
      export CC=mpiicc
      module purge && module load   git/2.8 impi/5.1.3  cmake/3.5  intel/16.0  anaconda/2


2017-05-18
------------------------------------------------------------------------
 ✔ fixed a bug in output.f90 which would allocate the array with the 
   proper size only for rank=0 (instead of coord_r = 0). That caused a
   glibc error
 ✔ All diagnostics tests passed


2017-04-27
------------------------------------------------------------------------
 ✔ Transition to GitHub completed (I'll be pushing the changes soon) but 
   head is still 212f85cde21e3e942e62.
 ✔ "parallel" namelist can be read from command line too, e.g.:
   magic.exe n_procs_r=4 n_procs_theta=2 input.nml
   will use n_procs_m from the parameter file and the n_procs_r n_procs_theta
   from command line. Command line overrides input.nml, if both are present.
 ✔ input file (e.g. input.nml) is now the ***last*** command line argument
   when calling magic (because of the new command line arguments)
 ✔ magic_wizard.py was updated to accept --extracmd argument, which passes
   a string directly onto magic.exe, for the supracited purpose.

This will make it easier to run diagnostics withouth having to consistently
manipulate the various input.nml from samples directory.


2017-04-24
------------------------------------------------------------------------
 ✔ there was a lot of trouble with the output. That is because multiple
   processes tried to open the same file simultaneously (since there are
   n_proc_phi x n_proc_theta processes whose coord_r = 0). Mostly, I have
   no idea if some of the computations done inside "if rank == 0" are to be
   later used, or if they are *merely* for outputing purpose! For this reason,
   in some of the occurrences I have replaced it for "if coord_r=0" and have
   introduced a "if rank == 0" before open/close/write. I have also enforced
   that l_save_out = .false. unless rank = 0. There might be problems lingering
   with respect to output!
 ✔ most of the occurrences of "rank" variable have been replaced by 
   "coord_r", which is basically the rank in comm_r
 ✔ most occurrences of MPI_COMM_WORLD have been replaced by comm_r
 ✔ comm_r created (in initialize_cartesian). Every computation is cloned
   now for each disjoint comm_r
 ✔ rank is now the rank in cart_comm, whereas coord_X are the coordinates
   in the cart_comm. The variable rank should still match the same rank as
   previously.
 ✔ added cart_comm
 ✔ added coord_r, coord_theta, coord_phi to replace "rank"
 ✔ added initialize_cartesian in parallel.f90
   

2017-04-19 and earlier
---------------------------------------------
As we discussed last year in the Skype call, as a first step, I was planning 
to parallelize the environment "very inefficiently", by merely cloning everything 
for every theta and phi. So, a run with 32 processes partitioned as 
(r,θ,φ)=(8,2,2) would yield 4 copies of the same computation, and each copy 
would only communicated with the 8 processes in the r direction. Once that 
would be settled, we could work on doing the real parallelization.

1) I have noticed that there is a problem with your ms2time routine in timing.f90. 
It seems that the integers do not have enough bytes. I've changed the msecSecond, 
msecMinute and msecHour variables to integer(lip), and modified the function to:

hours   =int(mSeconds/msecHour)
mSeconds=mSeconds-int(hours*msecHour,kind=lip)
minutes =int(mSeconds/msecMinute)
mSeconds=mSeconds-int(minutes*msecMinute,kind=lip)
seconds =int(mSeconds/msecSecond)
mSeconds=mSeconds-int(seconds*msecSecond,kind=lip)

2) which then seems to have the correct result. However, it prints things like 17000 
days or more, and I have no idea if that is expected. If this doesn't look correct, 
then I'll doublecheck the result of MPI_Wtime.

3) There are A LOT of temporary arrays being created as a result of the call to 
ZGETRS in algebra_lapack.f90 line 59. Have you looked into that?

4) There is quite a substantial amount of code which is executed only for 
process 0, if I understood it correctly, those are meant for diagnostics and 
checkpointing in most of the cases. Have you ever checked the loadbalance of 
this code?

5) I don't fully understand what is going on with nLMBs_per_rank, set in 
blocking.f90. It seems that it is always set to 1, am I correct (lines 139 
and 140)?

 
