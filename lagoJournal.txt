2017-06-21
------------------------------------------------------------------------
Module legendre_grid_to_spec will be dropped in favour of 
legendre_parallel for MPI version. I'll copy the functions as they are
needed and adapt them accordingly.

legendre_spec_to_grid module is overcomplicated because of the blocking
nThetaStart < nTheta < nThetaStart-1+sizeThetaB (implemented to favour 
cache-hit). 

Dunno if I should keep it or throw it all away. Cache-Hit will be 
secondary if it incurs in extra allreduces, and if I understood 
correctly, this is basically a whole bunch of inner-products (which 
require an allreduce in parallel cases).

Maybe for the MPI sake it would be better to group the leg_helper
vectors into a matrix and assign them pointers e.g. 
leg_helper%matrix(:,sR) instead of leg_helper%sR.

This would turn lots of things into a simple matrix-vector product,
and therefore linearly scalable. This is all speculation, maybe it will
be better to keep current structure.



2017-06-02
------------------------------------------------------------------------
The callstack for the do_iteration, starting from rIterThetaParallel is:

⚪ do_iteration_ThetaParallel (rIterThetaParallel:450)
⚪    loop                (rIterThetaParallel:557)
⚪    this%transform_to_grid_space (rIterThetaParallel:154)
⚪       legTFG           (legendre_spec_to_grid:27)
⚪          leg_helper_t%__ATTRIBUTES__    
⚪       loop             (legendre_spec_to_grid:181)
⚪       legTFGnomag      (legendre_spec_to_grid:733)
⚪          leg_helper_t%__ATTRIBUTES__    
⚪       fft_thetab       (fft:156)
⚪         fftJW          (fft:169)
⚪           fft99aJW     (fft:383)
⚪           wpass2JW     (fft:483)
⚪           wpass3JW     (fft:518)
⚪           wpass4JW     (fft:621)
⚪           wpass5JW     (fft:729)
⚪           fft99bJW     (fft:426)
⚪       v_rigid_boundary (nonlinear_bcs:195)
⚪    this%gsa%get_nl                      ?
⚪    this%transform_to_lm_space           ?
⚪    get_br_v_bcs                         ?
⚪    get_lorentz_torque                   ?
⚪    courant                              ?
⚪    graphOut_mpi                         ?
⚪    probe_out                            ?
⚪    get_helicity                         ?
⚪    get_visc_heat                        ?
⚪    get_nlBLayers                        ?
⚪    get_fluxes                           ?
⚪    get_perpPar                          ?
⚪    store_movie_frame                    ?
⚪    get_dtBLM                            ?
⚪    getTOnext                            ?
⚪    getTO                                ?
⚪    this%nl_lm%get_td                    ?
⚪    getTOfinish                          ?
⚪    get_dH_dtBLM                         ?



2017-05-31
------------------------------------------------------------------------
 ✔ Adding rThetaParallel module; at the moment, it merely merges
   rThetaBlocking with rThetaBlocking_seq. I will slowly adapt it from
   now on
 ✔ I'm validating the modifications with the following command
      ./magic_wizard.py --use-mpi --mpicmd=srun --extracmd="n_procs_r=4 n_procs_theta=2 n_procs_m=2" --nranks=16
   called from samples/ directory. The modules/environment variables are:
      export FC=mpiifort
      export CC=mpiicc
      module purge && module load   git/2.8 impi/5.1.3  cmake/3.5  intel/16.0  anaconda/2


2017-05-18
------------------------------------------------------------------------
 ✔ fixed a bug in output.f90 which would allocate the array with the 
   proper size only for rank=0 (instead of coord_r = 0). That caused a
   glibc error
 ✔ All diagnostics tests passed


2017-04-27
------------------------------------------------------------------------
 ✔ Transition to GitHub completed (I'll be pushing the changes soon) but 
   head is still 212f85cde21e3e942e62.
 ✔ "parallel" namelist can be read from command line too, e.g.:
   magic.exe n_procs_r=4 n_procs_theta=2 input.nml
   will use n_procs_m from the parameter file and the n_procs_r n_procs_theta
   from command line. Command line overrides input.nml, if both are present.
 ✔ input file (e.g. input.nml) is now the ***last*** command line argument
   when calling magic (because of the new command line arguments)
 ✔ magic_wizard.py was updated to accept --extracmd argument, which passes
   a string directly onto magic.exe, for the supracited purpose.

This will make it easier to run diagnostics withouth having to consistently
manipulate the various input.nml from samples directory.


2017-04-24
------------------------------------------------------------------------
 ✔ there was a lot of trouble with the output. That is because multiple
   processes tried to open the same file simultaneously (since there are
   n_proc_phi x n_proc_theta processes whose coord_r = 0). Mostly, I have
   no idea if some of the computations done inside "if rank == 0" are to be
   later used, or if they are *merely* for outputing purpose! For this reason,
   in some of the occurrences I have replaced it for "if coord_r=0" and have
   introduced a "if rank == 0" before open/close/write. I have also enforced
   that l_save_out = .false. unless rank = 0. There might be problems lingering
   with respect to output!
 ✔ most of the occurrences of "rank" variable have been replaced by 
   "coord_r", which is basically the rank in comm_r
 ✔ most occurrences of MPI_COMM_WORLD have been replaced by comm_r
 ✔ comm_r created (in initialize_cartesian). Every computation is cloned
   now for each disjoint comm_r
 ✔ rank is now the rank in cart_comm, whereas coord_X are the coordinates
   in the cart_comm. The variable rank should still match the same rank as
   previously.
 ✔ added cart_comm
 ✔ added coord_r, coord_theta, coord_phi to replace "rank"
 ✔ added initialize_cartesian in parallel.f90
   

2017-04-19 and earlier
---------------------------------------------
As we discussed last year in the Skype call, as a first step, I was planning 
to parallelize the environment "very inefficiently", by merely cloning everything 
for every theta and phi. So, a run with 32 processes partitioned as 
(r,θ,φ)=(8,2,2) would yield 4 copies of the same computation, and each copy 
would only communicated with the 8 processes in the r direction. Once that 
would be settled, we could work on doing the real parallelization.

1) I have noticed that there is a problem with your ms2time routine in timing.f90. 
It seems that the integers do not have enough bytes. I've changed the msecSecond, 
msecMinute and msecHour variables to integer(lip), and modified the function to:

hours   =int(mSeconds/msecHour)
mSeconds=mSeconds-int(hours*msecHour,kind=lip)
minutes =int(mSeconds/msecMinute)
mSeconds=mSeconds-int(minutes*msecMinute,kind=lip)
seconds =int(mSeconds/msecSecond)
mSeconds=mSeconds-int(seconds*msecSecond,kind=lip)

2) which then seems to have the correct result. However, it prints things like 17000 
days or more, and I have no idea if that is expected. If this doesn't look correct, 
then I'll doublecheck the result of MPI_Wtime.

3) There are A LOT of temporary arrays being created as a result of the call to 
ZGETRS in algebra_lapack.f90 line 59. Have you looked into that?

4) There is quite a substantial amount of code which is executed only for 
process 0, if I understood it correctly, those are meant for diagnostics and 
checkpointing in most of the cases. Have you ever checked the loadbalance of 
this code?

5) I don't fully understand what is going on with nLMBs_per_rank, set in 
blocking.f90. It seems that it is always set to 1, am I correct (lines 139 
and 140)?

 
